{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8b944a-1b90-4133-ae5e-3c3935c486b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Retrieval-Augmented Generation: Question Answering based on Custom Dataset\n",
    "\n",
    "Many use cases such as building a chatbot require text (text2text) generation models like **[BloomZ 7B1](https://huggingface.co/bigscience/bloomz-7b1)**, **[Flan T5 XXL](https://huggingface.co/google/flan-t5-xxl)**, and **[Flan T5 UL2](https://huggingface.co/google/flan-ul2)** to respond to user questions with insightful answers. The **BloomZ 7B1**, **Flan T5 XXL**, and **Flan T5 UL2** models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.\n",
    "\n",
    "<img src=\"rag.png\" max-width=\"1080\"/>\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines the power of pre-trained LLMs with information retrieval - enabling more accurate and context-aware responses. \n",
    "\n",
    "1. The first process in a chat bot is to generate embeddings. Typically you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store as your knowledge base. \n",
    "\n",
    "2. Second process is to retrieve relevant information from the knowledge base, and generate a response based on retrieved information and input query.\n",
    "\n",
    "In this notebook we will demonstrate how to use **[Falcon-7B-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)** to answer questions using a library of documents as a reference, by using document embeddings and retrieval. The embeddings are generated from [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2), a [sentence-transformers](https://www.sbert.net/) embedding model.\n",
    "\n",
    "**This notebook serves a template such that you can easily replace the example dataset by your own to build a custom question and asnwering application.**\n",
    "\n",
    "This notebook is tested with SageMaker Studio with following configuration:\n",
    "- Image: PyTorch 1.13 Python 3.9 CPU Optimized\n",
    "- Kernel: Python 3\n",
    "- Instance type: ml.t3.medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75636b0f-5f43-4302-a042-dd2243cc94ee",
   "metadata": {},
   "source": [
    "## Step 1. Deploy large language model (LLM) in SageMaker JumpStart\n",
    "\n",
    "To better illustrate the idea, let's first deploy the LLM model that are required to perform the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e543419-22f9-499f-8528-fec04ddf2db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker --quiet\n",
    "!pip install ipywidgets==7.0.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c193a1-3a6f-4074-b21b-48bf1604a8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "model_version = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61313d73-c3db-4bc8-9039-e773c35ec3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!Model huggingface-llm-falcon-7b-instruct-bf16 has been deployed successfully.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'huggingface-llm-falcon-7b-instruct-bf16'  # this is hard-coded\n",
    "instance_type = 'ml.g5.2xlarge'\n",
    "\n",
    "unix_time = int(time.time())\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-rag-{model_id}-{unix_time}\")\n",
    "inference_instance_type = instance_type\n",
    "\n",
    "# Retrieve the inference container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "model_inference = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    env={\n",
    "        'HF_MODEL_ID': '/opt/ml/model'\n",
    "    }\n",
    ")\n",
    "model_predictor_inference = model_inference.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "print(f\"Model {model_id} has been deployed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e19b9b-48ea-41f7-9b78-f9ad964afa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = 'jumpstart-example-rag-huggingface-llm-f-2023-09-05-08-41-58-340'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eff3784-7d6b-4846-8456-58d62e7cefe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read().decode(\"utf8\"))    \n",
    "    return model_predictions[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffcbf1-bb41-4a9a-8309-bee687bcba11",
   "metadata": {},
   "source": [
    "## Step 2. Ask a question to LLM without providing the context\n",
    "\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064717ee-1f90-4855-a49e-7e9be25a7eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being asked is --> Question: Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "Answer:\n",
      " Managed Spot Training can be used with instances that are available in the AWS Marketplace. These instances include Amazon Linux, Ubuntu, and CentOS. You can also use instances that are available in the AWS Marketplace, such as the Amazon EMR clusters.\n",
      "\n",
      "Note: Managed Spot Training is only available in the AWS Marketplace for the following instance types:\n",
      "- Amazon Linux 2.x\n",
      "- Ubuntu 2.x\n",
      "- CentOS 7.x\n",
      "- Amazon EMR clusters\n",
      "\n",
      "If you are using an instance that is not available in the AWS Marketplace, you can use a managed instance from AWS Marketplace.\n"
     ]
    }
   ],
   "source": [
    "# These are hyper-parameters; Hyperparameters are used before inferencing a model because they have a\n",
    "# direct impact on the performance of the resulting machine learning model. \n",
    "# Hyperparameters are used before inferencing a model because they control the behavior of the model, \n",
    "# and optimize its performance for the job at hand.\n",
    "# For this workshop, hyper parameters have been identified for you. \n",
    "# If you like, you can use some of these in the code below.\n",
    "# They will impact the behavior of your LLM response. \n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.2,\n",
    "    # \"stop\": [\"\\n\"]\n",
    "}\n",
    "\n",
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "print(f\"Question being asked is --> {prompt}\")\n",
    "\n",
    "payload = {\"inputs\": prompt, \"parameters\": parameters}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = query_endpoint_with_json_payload(payload, endpoint_name)\n",
    "response = parse_response(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea919696-8ae0-4087-9531-5aacf557d817",
   "metadata": {},
   "source": [
    "## Step 3. Improve the answer to the same question using prompt engineering with insightful context\n",
    "\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e75378-f63f-405b-a590-928f50fc2c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being asked is -- > \"Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "\n",
      "Question: Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "Managed Spot Training is currently supported on all instances supported in Amazon SageMaker. This includes the following instance types:\n",
      "\n",
      "- `m5.large`\n",
      "- `m5.xlarge`\n",
      "- `m5.2xlarge`\n",
      "- `m5.4xlarge`\n",
      "- `m5.8xlarge`\n",
      "- `m5.12xlarge`\n",
      "- `m5.16xlarge`\n",
      "- `m5.24xlarge`\n",
      "- `m5.32xlarge`\n",
      "- `m5.40xlarge`\n",
      "- `m5.48xlarge`\n",
      "- `m5.56xlarge`\n",
      "- `m5.64xlarge`\n",
      "- `m5.72xlarge`\n",
      "- `m5.8xlarge`\n",
      "- `m5.10xlarge`\n",
      "- `m5.12xlarge`\n",
      "- `m5.14xlarge`\n",
      "- `m5.16xlarge`\n",
      "- `m5.18xlarge`\n",
      "- `m5.20xlarge`\n",
      "- `m5.24xlarge`\n",
      "- `m5.26xlarge`\n",
      "- `m5.28xlarge`\n",
      "- `m5.30xlarge`\n",
      "- `\n"
     ]
    }
   ],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "print(f\"Question being asked is -- > {prompt}\")\n",
    "\n",
    "payload = {\"inputs\": prompt, \"parameters\": parameters}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = query_endpoint_with_json_payload(payload, endpoint_name)\n",
    "response = parse_response(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ae2c8-2241-443e-9aee-8639f82ec5ed",
   "metadata": {},
   "source": [
    "The output from tells us the chance to get the correct response significantly correlates with the insightful context you send into the LLM. \n",
    "\n",
    "Now, the question becomes where can I find the insightful context based on the user query? The answer is to use a pre-stored knowledge data base with retrieval augmented generation, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0807d-406d-48a1-886e-2d91db4ef511",
   "metadata": {},
   "source": [
    "## Step 4. Use RAG based approach to identify the correct documents, and use them and question to query LLM\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "- Generate embedings for each of document in the knowledge library with the HuggingFace sentence-transformers embedding model.\n",
    "- Identify top K most relevant documents based on user query.\n",
    "  - For a query of your interest, generate the embedding of the query using the same embedding model.\n",
    "  - Search the indexes of top K most relevant documents in the embedding space using the SageMaker KNN algorithm.\n",
    "  - Use the indexes to retrieve the corresponded documents.\n",
    "- Combine the retrieved documents with prompt and question and send them into LLM.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991139b-5dcc-4dbe-80bc-47393bab0470",
   "metadata": {},
   "source": [
    "### 4.1 Preparing the embedding model\n",
    "\n",
    "We'll start with initializing our embeddings model. In this notebook, we are using all-mpnet-base-v2 model. \n",
    "\n",
    "If you want, you can experiment with other embedding model, for example you can use GPT-J-6B embedding model which you can easily deploy with a few clicks from SageMaker JumpStart UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f5e222-236a-42fc-91af-904082e192d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.13.1+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.14.1+cpu)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (4.24.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649cac9e-9965-4b8c-9cbd-a9bc843eede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029542a-55c4-4c5d-b987-af0f3ea2b544",
   "metadata": {},
   "source": [
    "### 4.2. Generate embedings for each of document in the knowledge library with the embedding model.\n",
    "\n",
    "For the purpose of the demo we will use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) as knowledge library. The data are formatted in a CSV file with two columns Question and Answer. We use only the Answer column as the documents of knowledge library, from which relevant documents are retrieved based on a query.\n",
    "\n",
    "Each row in the CSV format dataset corresponds to a textual document. We will iterate each document to get its embedding vector via the all-mpnet-base-v2 embedding models. For your purpose, you can replace the example dataset of your own to build a custom question and answering application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79177b06-7b3d-49d9-939b-71de23ad14b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer\n",
       "0  Amazon SageMaker is a fully managed service to...\n",
       "1  For a list of the supported Amazon SageMaker A...\n",
       "2  Amazon SageMaker is designed for high availabi...\n",
       "3  Amazon SageMaker stores code in ML storage vol...\n",
       "4  Amazon SageMaker ensures that ML model artifac..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_knowledge = pd.read_csv(\"Amazon_SageMaker_FAQs.csv\", header=None, usecols=[1], names=[\"Answer\"])\n",
    "df_knowledge.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1369ef1b-8118-4e60-8f0a-72ef901cf0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectors = encoder.encode(df_knowledge['Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cfa636a-a6eb-4b01-bdea-db2af46d9418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e6bef-884f-444c-bf0e-b35e76fdc598",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.3. Index the embedding knowledge library\n",
    "\n",
    "We will store and match the embeddings using a vector database. In this notebook, we will showcase [FAISS](https://github.com/facebookresearch/faiss) which will be transient and in memory.\n",
    "\n",
    "In real-world scenarios, [Amazon Opensearch](https://aws.amazon.com/opensearch-service/) is a popular choice for vector DB. You may refer to this [blog](https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/) and follow the instructions to build your own RAG solution with Opensearch! Alternatively, you may also use [Amazon Kendra](https://aws.amazon.com/kendra/) for its built-in NLP capabilities and pre-trained domain knowledge. Refer to this [blog](https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/) for guided instructions on how to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2734158-7390-4c97-8033-6659f8ee92c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.9/site-packages (1.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f212065b-6ddc-4f9f-afef-80bb4494a99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "vector_dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(vector_dimension)\n",
    "faiss.normalize_L2(vectors)\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a8269-8339-4765-bb50-b0413a1cf927",
   "metadata": {},
   "source": [
    "### 4.4 Retrieve the most relevant documents\n",
    "\n",
    "Given the embedding of a query, we will query the endpoint to get the indexes of top K most relevant documents and use the indexes to retrieve the corresponded textual documents.\n",
    "\n",
    "Next, the textual documents are concatenated with maximum length of MAX_SECTION_LEN. This is to make sure the context we send into the prompt contains a good enough amount of information all the while not exceeding model's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d165ebc1-8ff9-4212-9b91-2a620271ab33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SECTION_LEN = 1500\n",
    "THRESHOLD = 0.5\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "\n",
    "def construct_context(\n",
    "    context_predictions_idx, \n",
    "    context_prediction_dist, \n",
    "    df_knowledge, \n",
    "    threshold, \n",
    "    context_length\n",
    ") -> str:\n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "\n",
    "    for index, dist in zip(context_predictions_idx, context_prediction_dist):\n",
    "        document_section = df_knowledge.loc[index]\n",
    "        chosen_sections_len += len(document_section) + 2\n",
    "        if dist > threshold:\n",
    "            break\n",
    "        chosen_sections.append(SEPARATOR + document_section.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    concatenated_doc = \"\".join(chosen_sections)[:context_length]\n",
    "    print(\n",
    "        f\"With maximum sequence length {MAX_SECTION_LEN}, \\\n",
    "selected top {len(chosen_sections)} document sections: {concatenated_doc}\"\n",
    "    )\n",
    "\n",
    "    return concatenated_doc, len(chosen_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9eaaa-f359-425e-8ff6-bb2bbc531137",
   "metadata": {},
   "source": [
    "Now we can convert question to embedding, and search for the most relevant documents. In this example, we will retrieve top 5 most relevant documents (k=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc128948-60b7-499b-a129-1f1938ba1b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "question = 'Which instances can I use with Managed Spot Training in SageMaker?'\n",
    "search_vector = encoder.encode(question)\n",
    "_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d522c09d-c56e-4816-9249-b807a32e295e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.23976025, 0.35568824, 0.4087937 , 0.51908976, 0.5701388 ]],\n",
       "       dtype=float32),\n",
       " array([[90, 84, 91, 87, 85]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "distances, indices = index.search(_vector, k=k)\n",
    "distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "610a1874-10f3-4824-9f7a-ab50a224d5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1500, selected top 3 document sections: \n",
      "  Managed Spot Training can be used with all instances supported in Amazon SageMaker.\n",
      "* Managed Spot Training with Amazon SageMaker lets you train your ML models using Amazon EC2 Spot instances, while reducing the cost of training your models by up to 90%.\n",
      "  Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n"
     ]
    }
   ],
   "source": [
    "context_retrieve, num_context_doc = construct_context(\n",
    "    indices[0], \n",
    "    distances[0], \n",
    "    df_knowledge[\"Answer\"], \n",
    "    threshold=THRESHOLD,\n",
    "    context_length=MAX_SECTION_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744b97e-222b-4401-9748-35345ede2fe7",
   "metadata": {},
   "source": [
    "### 4.5 Combine the retrieved documents, prompt, and question to query the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cd63dc9-1372-4e7f-910f-479d4eaea1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being asked is -- > \"Context: \n",
      "  Managed Spot Training can be used with all instances supported in Amazon SageMaker.\n",
      "* Managed Spot Training with Amazon SageMaker lets you train your ML models using Amazon EC2 Spot instances, while reducing the cost of training your models by up to 90%.\n",
      "  Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "\n",
      "Question: Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "1. Amazon EC2 instances with a minimum of 1 vCPU and 2GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "2. Amazon EC2 instances with a minimum of 2 vCPUs and 4GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "3. Amazon EC2 instances with a minimum of 4 vCPUs and 8GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "4. Amazon EC2 instances with a minimum of 8 vCPUs and 16GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "5. Amazon EC2 instances with a minimum of 16 vCPUs and 32GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "6. Amazon EC2 instances with a minimum of 32 vCPUs and 64GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "7. Amazon EC2 instances with a minimum of 64 vCPUs and 128GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "8. Amazon EC2 instances with a minimum of 128 vCPUs and 256GB of memory can be used with Managed Spot Training in SageMaker.\n",
      "\n",
      "9. Amazon EC2 instances with a minimum of 256 vCPUs and 512GB of memory can be used with Managed Spot\n"
     ]
    }
   ],
   "source": [
    "context = context_retrieve\n",
    "\n",
    "prompt = f\"\"\"\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "print(f\"Question being asked is -- > {prompt}\")\n",
    "\n",
    "payload = {\"inputs\": prompt, \"parameters\": parameters}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = query_endpoint_with_json_payload(payload, endpoint_name)\n",
    "response = parse_response(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f0299-a877-40d9-9685-87afc6c879b9",
   "metadata": {},
   "source": [
    "## 4.6 Clean Up\n",
    "\n",
    "Uncomment below cell to delete the endpoint after testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b57b1928-c3ed-4ee4-b9a7-f8c95da7c97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_predictor_inference.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
